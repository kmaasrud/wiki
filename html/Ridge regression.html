<!DOCTYPE HTML>
<html color-mode="light">
<head>
<title>Ridge regression</title>
<meta charset="UTF-8">
<meta content="text/html; charset=utf-8" http-equiv="Content-Type">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="theme-color" content="#ffffff" />
<!-- Normalize -->
<link rel="stylesheet" href="css/normalize.min.css">
<!-- Prism -->
<link rel="stylesheet" href="css/prism.min.css">
<script src="js/prism.min.js"></script>
<!-- KaTeX -->
<link rel="stylesheet" href="css/katex.min.css">
<script src="js/katex.min.js"></script>
<script src="js/auto-render.min.js"></script>
<script>
document.addEventListener("DOMContentLoaded", function() {
renderMathInElement(document.body, {
delimiters: [
{left: '$$', right: '$$', display: true},
{left: '$', right: '$', display: false},
{left: '\\(', right: '\\)', display: false},
{left: '\\[', right: '\\]', display: true}
],
});
});
</script>
<!-- mdzk stylesheet -->
<link rel="stylesheet" href="css/mdzk.css">
</head>
<body>
<label id="theme-switch" class="theme-switch" for="checkbox_theme">
<input type="checkbox" id="checkbox_theme">
<span></span>
</label>
<main>
<header>

<h1>Ridge regression</h1>
</header>
<article>
<p><strong>Ridge regression</strong> is a <a href="Linear%20regression.html">regression</a> method based on reducing the size of the regression coefficients, and thus increasing the <a href="Bias.html">bias</a> of the model.</p>
<p>Like in <a href="Ordinary%20least%20squares%20regression.html">OLS regresion</a>, the <a href="Cost%20function.html">cost function</a> is an <a href="Residual%20sum%20of%20squares.html">RSS function</a>, but this time with an extra term pertaining to the shrinkage of each regression coefficient:</p>
<p>$$ S(\beta,\lambda) = \text{RSS}(\beta) + \lambda \sum_{j=1}^p\beta_j^2 = \sum_{i=1}^N (y_i - \hat y_i)^2 + \lambda \sum_{j=1}^p\beta_j^2 ,$$</p>
<p>$$ S(\beta, \lambda) = (\mathbf y - \mathbf X\beta)^T(\mathbf y - \mathbf X\beta) + \lambda \beta^T\beta $$</p>
<p>Here, $\lambda \ge 0$ is a parameter that controls how much the $\beta$â€˜s are shrunkâ€™; the greater the $\lambda$, the more shrinkage. The <em>Ridge estimator</em> is of course now</p>
<p>$$ \hat \beta^{\text{ridge}} = \underset{\beta}{\arg \min}\ S(\beta,\lambda) .$$</p>
<p>Like explained in the <a href="Ordinary%20least%20squares%20regression.html">page about OLS</a>, the above minimization problem can be written on matrix form as <sup class="footnote-reference"><a href="#closed-form">1</a></sup></p>
<p>$$ \hat \beta^{\text{ridge}} = (\mathbf X^T\mathbf X + \lambda \mathbf I)^{-1}\mathbf X^T\mathbf y ,$$</p>
<p>where $\mathbf I$ is the $p\times p$ identity matrix. Notice that when adding a positive constant to the diagonal (or <em>ridge</em> ðŸ˜‰), the matrix inside the parentheses will always be invertible. This was one of the main motivating factors for Ridge regression. Its also worth noting that with <a href="Orthonormality.html">orthonormal</a>, inputs, Ridge regression is just a scaled version of the <a href="Ordinary%20least%20squares%20regression.html">OLS regression</a>, $\hat \beta^{\text{ridge}} = \frac{1}{1 + \lambda}\hat \beta$.</p>
<p>Now what is the whole idea behind shrinking the regression coefficients? Well, its quite clear that when $\lambda = 0$ the degrees of freedom of the fit is $\text{df}(\lambda) = p$, since this is the number of parameters. However, as $\lambda \rightarrow \infty$, then also $\text{df}(\lambda) \rightarrow 0$, since we gradually reduce the influence the different parameters have on the fit. So the goal is essentially to reduce the influence of unimportant parameters, and thus reduce <a href="Overfitting.html">overfitting</a>. This ties directly in with reducing the <a href="Variance.html">variance</a> by increasing the <a href="Bias.html">bias</a>, following the <a href="Bias-variance%20tradeoff.html">Bias-variance tradeoff</a>. ^c82391</p>
<div class="footnote-definition" id="closed-form"><sup class="footnote-definition-label">1</sup>
<p>The Ridge regressor can be expressed in a closed-form expression, but for other regression methods, this might not be the case. Therefore, here is a more general formula:
$$ \hat \beta^{\text{ridge}} = \underset{\beta}{\arg \min} \begin{Bmatrix}\sum_{i=1}^N\left(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j\right)^2 + \lambda \sum_{j=1}^p \beta_j^2 \end{Bmatrix} .$$</p>
</div>

</article>
<footer class="backlinks">
<p>

<a href="Linear regression.html">Linear regression</a>

<a href="Lasso regression.html">Lasso regression</a>

</p>
</footer>
</main>
<script>
// Determines if the user has a set theme
function detectColorScheme(){
var theme="light"; // default to light
// Local storage is used to override OS theme settings
if (localStorage.getItem("color-mode")) {
if (localStorage.getItem("color-mode") == "dark") {
var theme = "dark";
}
} else if (!window.matchMedia) {
// matchMedia method not supported
return false;
} else if (window.matchMedia("(prefers-color-scheme: dark)").matches) {
// OS theme setting detected as dark
var theme = "dark";
}
// Dark theme preferred, set document with a `data-theme` attribute
if (theme == "dark") {
document.documentElement.setAttribute("color-mode", "dark");
}
}
detectColorScheme();
// Identify the toggle switch HTML element
const toggleSwitch = document.querySelector('#theme-switch input[type="checkbox"]');
// Function that changes the theme, and sets a localStorage variable to track the theme between page loads
function switchTheme(e) {
if (e.target.checked) {
localStorage.setItem("color-mode", "dark");
document.documentElement.setAttribute("color-mode", "dark");
toggleSwitch.checked = true;
} else {
localStorage.setItem("color-mode", "light");
document.documentElement.setAttribute("color-mode", "light");
toggleSwitch.checked = false;
}
}
// Listener for changing themes
toggleSwitch.addEventListener('click', switchTheme, false);
// Pre-check the dark-theme checkbox if dark-theme is set
if (document.documentElement.getAttribute("color-mode") == "dark"){
toggleSwitch.checked = true;
}
</script>
</body>
</html>