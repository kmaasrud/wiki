---
date = "2020-08-24T10:15"
tags = ["lecture", "MEK2200"]
---

**Vær obs på vektornotasjon**. Vi kan skrive $\vec{A}$ eller $\mathbb{A}$. Bruker vi  [[Index notation|indeksnotasjon]]: $A_i$, er $i = 1,2,3$ implisitt. Vi har da:

$$\vec{A} = A_i \vec{i}_i = \sum_{i=1}^{3}A_i \vec{i}_i,$$

hvor $\vec{i}_i$ er enhetsvektoren. Dette går videre over til [[Inner product|indreproduktet]]:

$$\vec{A}\cdot\vec{B} = A_i\vec{i}_i\cdot B_j \vec{i}_j = A_iB_i (\vec{i}_i\cdot\vec{i}_j) = A_iB_i \delta_{ij},$$

hvor $\delta_{ij}$ er [[Kronecker delta]]. Et nyttig resultat fra dette, er at $C_i \delta_{ij} = C_j$. 

## Nyttige resultat

$$\nabla \beta = \frac{\partial \beta}{\partial \vec{i}_i}$$

$$\nabla \cdot \vec{A} = \frac{\partial A_i}{\partial \vec{i}_i}$$

$$\nabla^2 \beta = \frac{\partial^2\beta}{\partial \vec{i}_i\partial \vec{i}_j}$$

***

Vi vil jobbe med [[Tensor|tensorer]]. Disse er en generalisering av skalarer/vektorer/matriser osv. Her vil også [[Index notation|indeksnotasjon]] være nyttig. Produktet mellom en vektor og en matrise, blir eksempelvis

$$P\vec{A} = A_i P_{ij} = A_1 P_{1j} + A_2 P_{2j} + A_3P_{3j} = M_j,$$

hvor $M_j$ er en én-dimensjonal tensor (vektor). Vi har også

$$P_{ij}A_i = P_{i1}A_1 + P_{i2}A_2 + P_{i3}A_3 = N_i.$$

Hvis $P$ er [[Symmetric matrix|symmetrisk]] - dvs. $P_{ij} = P_{ji}$ - ser vi at 

$$A_i P_{ij} = A_iP_{ji} = P_{ji}A_i = P_{ij}A_j.$$

Videre operasjoner ved hjelp av indeksnotasjon:

- Matrisemultiplikasjon: $S_{ij}T_{jk} = Q_{ik}$
- [[Hadamard product|Elementvis multiplikasjon]]: $S_{ij}T_{ij} = \alpha$

***

Vi kan stykke opp en matrise på litt samme måte som en vektor, ved å si $P_{ij} = P_{ij}\vec{i}_i\vec{i}_j$. Bruker vi dette i en multiplikasjonsprosess, vil vi se

$$\vec{A}P = A_k\vec{i}_k \cdot P_{ij} \vec{i}_i \vec{j}_j = A_k P_{ij} (\vec{i}_k \cdot \vec{i}_i)\cdot \vec{i}_j = A_k P_{ij} \delta_{ki} \vec{i}_j = A_i P_{ij} \vec{i}_j$$