---
date = "2020-08-24T10:15"
tags = ["lecture", "MEK2200"]
---

**Vær obs på vektornotasjon**. Vi kan skrive $\mathbf{A}$, $\mathbf{A}$ eller $A_i$. Sistenevnte kalles [[Index notation|indeksnotasjon]], hvor vi antar $i = 1,2,3$ implisitt. Vi har at:

$$\mathbf{A} = A_i \mathbf{i}_i = \sum_{i=1}^{3}A_i \mathbf{i}_i,$$

hvor $\mathbf{i}_1, \mathbf{i}_2$ og $\mathbf{i}_3$ er de tre kartesiske enhetsvektorene. Dette går videre over til [[Inner product|indreproduktet]]:

$$\mathbf{A}\cdot\mathbf{B} = A_i\mathbf{i}_i\cdot B_j \mathbf{i}_j = A_iB_i (\mathbf{i}_i\cdot\mathbf{i}_j) = A_iB_i \delta_{ij},$$

hvor $\delta_{ij}$ er [[Kronecker delta]]. Et nyttig resultat fra dette, er at $C_i \delta_{ij} = C_j$. 

## Nyttige resultat

$$\nabla \beta = \frac{\partial \beta}{\partial x_i}$$

$$\nabla \cdot \mathbf{A} = \frac{\partial A_i}{\partial x_i}$$

$$\nabla^2 \beta = \frac{\partial^2\beta}{\partial x_i\partial x_i}$$

***

Vi vil jobbe med [[Tensor|tensorer]]. Disse er en generalisering av skalarer/vektorer/matriser osv. Her vil også [[Index notation|indeksnotasjon]] være nyttig. *Pre-multiplikasjon* mellom en vektor og en matrise, blir eksempelvis

$$\mathbf{A}\cdot\mathcal P = A_i P_{ij} = A_1 P_{1j} + A_2 P_{2j} + A_3P_{3j} = M_j,$$

hvor $M_j$ er en én-dimensjonal tensor (vektor). Vi har også *post-multiplikasjon*:

$$\mathcal P \cdot \mathbf A = P_{ij}A_i = P_{i1}A_1 + P_{i2}A_2 + P_{i3}A_3 = N_i.$$

Hvis $\mathcal P$ er [[Symmetric matrix|symmetrisk]] - dvs. $P_{ij} = P_{ji}$ - er disse like, nemlig 

$$A_i P_{ij} = A_iP_{ji} = P_{ji}A_i = P_{ij}A_j.$$

Videre operasjoner ved hjelp av indeksnotasjon:

- Matrisemultiplikasjon: $S_{ij}T_{jk} = Q_{ik}$
- Elementvis multiplikasjon og summering: $S_{ij}T_{ij} = \alpha$

***

Indeksnotasjonen er begrenset av at vi definerer over kartesiske koordinater. Skal andre koordinatsystemer benyttes (eksempelvis sfærisk e.l.) må *dyade-formalismen* innføres.

Vi kan stykke opp en matrise på litt samme måte som en vektor, ved å si $P_{ij} = P_{ij}\mathbf{i}_i\mathbf{i}_j$. Dette kalles *dyadeform*. Bruker vi dette i en multiplikasjonsprosess, vil vi se

$$\mathbf{A} \cdot\mathcal P = A_k\mathbf{i}_k \cdot P_{ij} \mathbf{i}_i \mathbf{j}_j = A_k P_{ij} (\mathbf{i}_k \cdot \mathbf{i}_i) \mathbf{i}_j = A_k P_{ij} \delta_{ki} \mathbf{i}_j = A_i P_{ij} \mathbf{i}_j.$$

Tilsvarende for post-multiplikasjon:

$$\mathcal P \cdot\mathbf{A}  = P_{ij} \mathbf{i}_i \mathbf{j}_j \cdot A_k\mathbf{i}_k = P_{ij} A_j \mathbf{i}_i (\mathbf{i}_j \cdot \mathbf{i}_k)\cdot = P_{ij} A_k \mathbf{i}_i\delta_{jk} = P_{ij} A_j \mathbf{i}_i.$$